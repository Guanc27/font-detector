{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Font Embedding Model Training (Phase 2)\n",
    "\n",
    "This notebook fine-tunes OpenCLIP ViT-B-32 on your font dataset to create font embeddings.\n",
    "\n",
    "**Prerequisites:** You must have the `font_dataset/` folder (with `metadata.json` and `samples/`) ready to upload.\n",
    "\n",
    "## Workflow\n",
    "1. Set your project path in the config cell below\n",
    "2. Upload your `font_dataset/` folder to Google Drive\n",
    "3. Run all cells\n",
    "4. Download the trained `best_model.pt` back to your local project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Configuration\n",
    "\n",
    "**Change `PROJECT_DIR` to match where you placed `font_dataset/` inside Google Drive.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# === CONFIGURATION - EDIT THIS ===\n",
    "#\n",
    "# Path inside Google Drive where your project lives.\n",
    "# After mounting, Drive is at /content/drive/MyDrive/\n",
    "# Example: if you uploaded font_dataset/ into Drive > proj1_check_fonts > check_fonts\n",
    "#   then set PROJECT_DIR = \"/content/drive/MyDrive/proj1_check_fonts/check_fonts\"\n",
    "#\n",
    "PROJECT_DIR = \"/content/drive/MyDrive/proj1_check_fonts/check_fonts\"\n",
    "\n",
    "# Training hyperparameters\n",
    "MODEL_NAME   = \"ViT-B-32\"\n",
    "PRETRAINED   = \"openai\"\n",
    "EPOCHS       = 15\n",
    "LEARNING_RATE = 1e-4\n",
    "BATCH_SIZE   = 32\n",
    "AUGMENTATION = True   # Set False to disable augmentation\n",
    "NUM_WORKERS  = 2\n",
    "\n",
    "# Derived paths (no need to edit)\n",
    "DATASET_DIR  = f\"{PROJECT_DIR}/font_dataset\"\n",
    "METADATA     = f\"{PROJECT_DIR}/font_dataset/metadata.json\"\n",
    "SAVE_DIR     = f\"{PROJECT_DIR}/models\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Mount Google Drive & Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount(\"/content/drive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q open-clip-torch pillow tqdm numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify GPU is available\n",
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available:  {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU:             {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"VRAM:            {torch.cuda.get_device_properties(0).total_mem / 1e9:.1f} GB\")\n",
    "else:\n",
    "    print(\"WARNING: No GPU detected. Go to Runtime > Change runtime type > T4 GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Verify dataset exists\n",
    "dataset_path = Path(DATASET_DIR)\n",
    "metadata_path = Path(METADATA)\n",
    "\n",
    "assert dataset_path.exists(), f\"Dataset directory not found: {DATASET_DIR}\\nMake sure you uploaded font_dataset/ to the correct Drive path.\"\n",
    "assert metadata_path.exists(), f\"metadata.json not found: {METADATA}\"\n",
    "\n",
    "import json\n",
    "with open(METADATA, \"r\") as f:\n",
    "    meta = json.load(f)\n",
    "print(f\"Dataset verified: {meta['num_fonts']} fonts, \"\n",
    "      f\"{sum(fd['num_samples'] for fd in meta['fonts'])} total samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define Model & Dataset Classes\n",
    "\n",
    "These are identical to `train_embedding_model.py` so that checkpoints are compatible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from pathlib import Path\n",
    "from PIL import Image, ImageFilter, ImageEnhance\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import random\n",
    "import open_clip\n",
    "\n",
    "\n",
    "# ── Augmentation helpers ──────────────────────────────────────────\n",
    "\n",
    "class GaussianBlur:\n",
    "    def __init__(self, radius_range=(0.5, 2.0)):\n",
    "        self.radius_range = radius_range\n",
    "    def __call__(self, img):\n",
    "        radius = random.uniform(*self.radius_range)\n",
    "        return img.filter(ImageFilter.GaussianBlur(radius=radius))\n",
    "\n",
    "\n",
    "class AddGaussianNoise:\n",
    "    def __init__(self, mean=0, std_range=(5, 25)):\n",
    "        self.mean = mean\n",
    "        self.std_range = std_range\n",
    "    def __call__(self, img):\n",
    "        std = random.uniform(*self.std_range)\n",
    "        arr = np.array(img).astype(np.float32)\n",
    "        noise = np.random.normal(self.mean, std, arr.shape)\n",
    "        arr = np.clip(arr + noise, 0, 255).astype(np.uint8)\n",
    "        return Image.fromarray(arr)\n",
    "\n",
    "\n",
    "class RandomPerspective:\n",
    "    def __init__(self, distortion_scale=0.1, p=0.3):\n",
    "        self.distortion_scale = distortion_scale\n",
    "        self.p = p\n",
    "    def __call__(self, img):\n",
    "        if random.random() < self.p:\n",
    "            w, h = img.size\n",
    "            startpoints = [[0, 0], [w, 0], [w, h], [0, h]]\n",
    "            endpoints = []\n",
    "            for x, y in startpoints:\n",
    "                dx = random.uniform(-w * self.distortion_scale, w * self.distortion_scale)\n",
    "                dy = random.uniform(-h * self.distortion_scale, h * self.distortion_scale)\n",
    "                endpoints.append([x + dx, y + dy])\n",
    "            coeffs = self._get_perspective_coeffs(startpoints, endpoints)\n",
    "            return img.transform(img.size, Image.Transform.PERSPECTIVE, coeffs, Image.Resampling.BILINEAR)\n",
    "        return img\n",
    "    def _get_perspective_coeffs(self, src, dst):\n",
    "        matrix = []\n",
    "        for p1, p2 in zip(src, dst):\n",
    "            matrix.append([p1[0], p1[1], 1, 0, 0, 0, -p2[0]*p1[0], -p2[0]*p1[1]])\n",
    "            matrix.append([0, 0, 0, p1[0], p1[1], 1, -p2[1]*p1[0], -p2[1]*p1[1]])\n",
    "        A = np.array(matrix, dtype=np.float32)\n",
    "        B = np.array(dst, dtype=np.float32).reshape(8)\n",
    "        res = np.linalg.lstsq(A, B, rcond=None)[0]\n",
    "        return np.concatenate([res, [1.0]]).reshape(9)\n",
    "\n",
    "\n",
    "# ── Dataset ────────────────────────────────────────────────────────\n",
    "\n",
    "class FontDataset(Dataset):\n",
    "    def __init__(self, dataset_dir, metadata_file, transform=None, augment=False):\n",
    "        self.dataset_dir = Path(dataset_dir)\n",
    "        self.transform = transform\n",
    "        self.augment = augment\n",
    "\n",
    "        with open(metadata_file, \"r\") as f:\n",
    "            metadata = json.load(f)\n",
    "\n",
    "        self.samples = []\n",
    "        self.font_to_idx = {}\n",
    "        self.idx_to_font = {}\n",
    "        font_idx = 0\n",
    "        found = missing = 0\n",
    "\n",
    "        for font_data in metadata[\"fonts\"]:\n",
    "            font_name = font_data[\"name\"]\n",
    "            if font_name not in self.font_to_idx:\n",
    "                self.font_to_idx[font_name] = font_idx\n",
    "                self.idx_to_font[font_idx] = font_name\n",
    "                font_idx += 1\n",
    "            label = self.font_to_idx[font_name]\n",
    "            for sample in font_data[\"samples\"]:\n",
    "                sample_path = sample[\"path\"].replace(\"\\\\\\\\\", \"/\").replace(\"\\\\\", \"/\")\n",
    "                image_path = self.dataset_dir / sample_path\n",
    "                if image_path.exists():\n",
    "                    self.samples.append((str(image_path), label))\n",
    "                    found += 1\n",
    "                else:\n",
    "                    missing += 1\n",
    "\n",
    "        self.num_fonts = len(self.font_to_idx)\n",
    "        print(f\"Loaded {found} samples from {self.num_fonts} fonts  (missing: {missing})\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path, label = self.samples[idx]\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        if self.augment:\n",
    "            image = self._apply_augmentation(image)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "    def _apply_augmentation(self, image):\n",
    "        if random.random() < 0.5:\n",
    "            image = image.rotate(random.uniform(-5, 5), fillcolor=\"white\", resample=Image.BILINEAR)\n",
    "        if random.random() < 0.5:\n",
    "            image = ImageEnhance.Brightness(image).enhance(random.uniform(0.8, 1.2))\n",
    "        if random.random() < 0.5:\n",
    "            image = ImageEnhance.Contrast(image).enhance(random.uniform(0.8, 1.2))\n",
    "        if random.random() < 0.3:\n",
    "            image = GaussianBlur(radius_range=(0.5, 1.5))(image)\n",
    "        if random.random() < 0.3:\n",
    "            image = AddGaussianNoise(std_range=(5, 15))(image)\n",
    "        if random.random() < 0.2:\n",
    "            image = RandomPerspective(distortion_scale=0.05, p=1.0)(image)\n",
    "        return image\n",
    "\n",
    "\n",
    "def split_dataset(dataset, train_ratio=0.7, val_ratio=0.15):\n",
    "    random.seed(42); np.random.seed(42); torch.manual_seed(42)\n",
    "    indices = list(range(len(dataset)))\n",
    "    random.shuffle(indices)\n",
    "    train_end = int(len(dataset) * train_ratio)\n",
    "    val_end   = train_end + int(len(dataset) * val_ratio)\n",
    "    train_ds = Subset(dataset, indices[:train_end])\n",
    "    val_ds   = Subset(dataset, indices[train_end:val_end])\n",
    "    test_ds  = Subset(dataset, indices[val_end:])\n",
    "    print(f\"Split: train={len(train_ds)}  val={len(val_ds)}  test={len(test_ds)}\")\n",
    "    return train_ds, val_ds, test_ds\n",
    "\n",
    "\n",
    "print(\"Classes defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Dataset & Prepare DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load full dataset (no transforms yet -- we add them after creating the model)\n",
    "dataset = FontDataset(DATASET_DIR, METADATA, transform=None)\n",
    "train_ds, val_ds, test_ds = split_dataset(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Initialize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "# Load OpenCLIP\n",
    "model, _, preprocess = open_clip.create_model_and_transforms(\n",
    "    MODEL_NAME, pretrained=PRETRAINED, device=device\n",
    ")\n",
    "model = model.to(device)\n",
    "\n",
    "# Freeze all vision params, unfreeze last 2 transformer blocks\n",
    "for param in model.visual.parameters():\n",
    "    param.requires_grad = False\n",
    "if hasattr(model.visual, \"transformer\"):\n",
    "    for param in model.visual.transformer.resblocks[-2:].parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "# Classification head\n",
    "embedding_dim = model.visual.output_dim if hasattr(model.visual, \"output_dim\") else 512\n",
    "num_fonts = dataset.num_fonts\n",
    "classifier = nn.Linear(embedding_dim, num_fonts).to(device)\n",
    "\n",
    "trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "trainable += sum(p.numel() for p in classifier.parameters())\n",
    "total = sum(p.numel() for p in model.parameters()) + sum(p.numel() for p in classifier.parameters())\n",
    "print(f\"Trainable parameters: {trainable:,} / {total:,}  ({100*trainable/total:.1f}%)\")\n",
    "print(f\"Fonts: {num_fonts}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply transforms to dataset splits\n",
    "train_ds.dataset.transform = preprocess\n",
    "train_ds.dataset.augment = AUGMENTATION\n",
    "val_ds.dataset.transform = preprocess\n",
    "val_ds.dataset.augment = False\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,  num_workers=NUM_WORKERS, pin_memory=True)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "\n",
    "print(f\"Train batches: {len(train_loader)}  |  Val batches: {len(val_loader)}\")\n",
    "print(f\"Augmentation: {'ON' if AUGMENTATION else 'OFF'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer & loss\n",
    "trainable_params = [p for p in model.parameters() if p.requires_grad]\n",
    "trainable_params += list(classifier.parameters())\n",
    "\n",
    "optimizer = torch.optim.AdamW(trainable_params, lr=LEARNING_RATE, weight_decay=0.01)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "save_path = Path(SAVE_DIR)\n",
    "save_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "best_val_acc = 0.0\n",
    "history = {\"train_loss\": [], \"train_acc\": [], \"val_loss\": [], \"val_acc\": []}\n",
    "\n",
    "print(f\"Training {EPOCHS} epochs  |  LR={LEARNING_RATE}  |  Batch={BATCH_SIZE}\")\n",
    "print(f\"Saving checkpoints to: {SAVE_DIR}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    # ── Train ─────────────────────────────────────────────\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = total = 0\n",
    "\n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS} [train]\")\n",
    "    for images, labels in pbar:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        features = model.encode_image(images)\n",
    "        logits = classifier(features)\n",
    "        loss = criterion(logits, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        _, preds = logits.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        pbar.set_postfix(loss=f\"{loss.item():.4f}\", acc=f\"{100*correct/total:.1f}%\")\n",
    "\n",
    "    train_loss = running_loss / len(train_loader)\n",
    "    train_acc  = 100 * correct / total\n",
    "\n",
    "    # ── Validate ──────────────────────────────────────────\n",
    "    model.eval()\n",
    "    val_loss_sum = 0.0\n",
    "    val_correct = val_total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            features = model.encode_image(images)\n",
    "            logits = classifier(features)\n",
    "            loss = criterion(logits, labels)\n",
    "            val_loss_sum += loss.item()\n",
    "            _, preds = logits.max(1)\n",
    "            val_total += labels.size(0)\n",
    "            val_correct += (preds == labels).sum().item()\n",
    "\n",
    "    val_loss = val_loss_sum / len(val_loader)\n",
    "    val_acc  = 100 * val_correct / val_total\n",
    "\n",
    "    # ── Log ───────────────────────────────────────────────\n",
    "    history[\"train_loss\"].append(train_loss)\n",
    "    history[\"train_acc\"].append(train_acc)\n",
    "    history[\"val_loss\"].append(val_loss)\n",
    "    history[\"val_acc\"].append(val_acc)\n",
    "\n",
    "    marker = \"\"\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save({\n",
    "            \"epoch\": epoch,\n",
    "            \"model_state_dict\": model.state_dict(),\n",
    "            \"classifier_state_dict\": classifier.state_dict(),\n",
    "            \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "            \"val_acc\": val_acc,\n",
    "            \"num_fonts\": num_fonts,\n",
    "        }, save_path / \"best_model.pt\")\n",
    "        marker = \"  << saved best\"\n",
    "\n",
    "    print(f\"Epoch {epoch+1:>2}/{EPOCHS}  \"\n",
    "          f\"train_loss={train_loss:.4f}  train_acc={train_acc:.1f}%  \"\n",
    "          f\"val_loss={val_loss:.4f}  val_acc={val_acc:.1f}%{marker}\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(f\"Training complete.  Best val accuracy: {best_val_acc:.1f}%\")\n",
    "print(f\"Checkpoint saved to: {save_path / 'best_model.pt'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "epochs_range = range(1, len(history[\"train_loss\"]) + 1)\n",
    "\n",
    "ax1.plot(epochs_range, history[\"train_loss\"], label=\"Train\")\n",
    "ax1.plot(epochs_range, history[\"val_loss\"], label=\"Val\")\n",
    "ax1.set_xlabel(\"Epoch\"); ax1.set_ylabel(\"Loss\"); ax1.set_title(\"Loss\")\n",
    "ax1.legend()\n",
    "\n",
    "ax2.plot(epochs_range, history[\"train_acc\"], label=\"Train\")\n",
    "ax2.plot(epochs_range, history[\"val_acc\"], label=\"Val\")\n",
    "ax2.set_xlabel(\"Epoch\"); ax2.set_ylabel(\"Accuracy (%)\"); ax2.set_title(\"Accuracy\")\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(save_path / \"training_curves.png\", dpi=150)\n",
    "plt.show()\n",
    "print(f\"Saved plot to {save_path / 'training_curves.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Quick Sanity Check\n",
    "\n",
    "Load the saved checkpoint and verify it works on a random sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best checkpoint and run a quick test\n",
    "ckpt = torch.load(save_path / \"best_model.pt\", map_location=device)\n",
    "model.load_state_dict(ckpt[\"model_state_dict\"])\n",
    "classifier.load_state_dict(ckpt[\"classifier_state_dict\"])\n",
    "model.eval()\n",
    "\n",
    "# Pick 5 random samples from test set\n",
    "test_loader = DataLoader(test_ds, batch_size=5, shuffle=True, num_workers=0)\n",
    "images, labels = next(iter(test_loader))\n",
    "images = images.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    features = model.encode_image(images)\n",
    "    logits = classifier(features)\n",
    "    _, preds = logits.max(1)\n",
    "\n",
    "print(\"Sanity check on 5 test samples:\")\n",
    "for i in range(len(labels)):\n",
    "    true_font = dataset.idx_to_font[labels[i].item()]\n",
    "    pred_font = dataset.idx_to_font[preds[i].item()]\n",
    "    match = \"OK\" if labels[i] == preds[i] else \"WRONG\"\n",
    "    print(f\"  [{match:>5}]  True: {true_font:<25}  Predicted: {pred_font}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Download Checkpoint\n",
    "\n",
    "If you want to download `best_model.pt` directly from Colab (instead of grabbing it from Drive):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: download directly from Colab to your browser\n",
    "from google.colab import files\n",
    "files.download(str(save_path / \"best_model.pt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "1. Copy `best_model.pt` into your local project at `proj1_check_fonts/check_fonts/models/best_model.pt`\n",
    "2. Run Phase 3 (Vector DB) locally -- it does **not** require a GPU:\n",
    "   ```\n",
    "   python phase3_vector_db.py --checkpoint models/best_model.pt\n",
    "   ```\n",
    "3. Test search:\n",
    "   ```\n",
    "   python search_font.py --image path/to/query.png --top_k 5\n",
    "   ```"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}